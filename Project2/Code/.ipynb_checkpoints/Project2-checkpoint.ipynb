{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.utils import resample, shuffle\n",
    "\n",
    "\n",
    "from classTest import ClassTest\n",
    "\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "testObject = ClassTest(15)\n",
    "print(testObject.getLayers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def franke(x, y):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return term1 + term2 + term3 + term4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pval(order):\n",
    "    return int((order + 1) * (order + 2) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`designMatrix()` creates the design matrix by evaluating all of these terms in the polynomial as a function of input vectors x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit #makes the function faster\n",
    "def designMatrix(x, y, order):\n",
    "    n = x.size\n",
    "    p = int((order + 1) * (order + 2) / 2) # number of columns in X (can't use the function pval() due to @jit)\n",
    "    X = np.zeros((n, p))\n",
    "    feature = 0\n",
    "    for x_power in range(order + 1):\n",
    "        for y_power in range(order - x_power + 1):\n",
    "            X[:, feature] = x**x_power * y**y_power\n",
    "            feature += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data()` generates datapoints from the franke function with noise at random points (x, y), and returns the design matrix and z-values. We found that n = 100 is few enough datapoints to see overfitting, so we will use 100 datapoints for the franke function for the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 # datapoints\n",
    "def data(n, order):\n",
    "    x = np.random.rand(n) #an array of n unordered uniform random numbers from 0 to 1\n",
    "    y = np.random.rand(n)\n",
    "    noise = np.random.randn(n) / 10\n",
    "    \n",
    "    z = franke(x, y) + noise\n",
    "    X = designMatrix(x, y, order)\n",
    "    \n",
    "    return X, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scale()` scales training and test data according to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X_train, X_test):\n",
    "    scaler = StandardScaler() #subtracts mean from each feature and divides by the standard deviation\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_train[:, 0] = 1 # scaling removed the intercept terms\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test[:, 0] = 1\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KarlH\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def OLS(X, z, lmda=None, p=None):\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ z\n",
    "    return beta\n",
    "\n",
    "def ridge(X, z, lmda, p):\n",
    "    beta = np.linalg.inv(X.T @ X + lmda * np.eye(p)) @ X.T @ z\n",
    "    return beta\n",
    "\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def lasso(X, z, lmda, p):\n",
    "    clf = skl.Lasso(lmda)\n",
    "    clf.fit(X,z)\n",
    "    return clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SGD import SGDLinReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup of data\n",
    "order = 5 # max order of polynomials\n",
    "\n",
    "X, z = data(n, order)\n",
    "X_train, X_test, z_train, z_test = train_test_split(X, z, test_size = 0.2)\n",
    "X_train, X_test = scale(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on training data = 0.0131\n",
      "Mean squared error = 0.0277\n"
     ]
    }
   ],
   "source": [
    "beta = np.zeros(pval(order))\n",
    "# OLS regression\n",
    "for i in range(10000):\n",
    "    beta = SGDLinReg(X_train, beta, z_train, 0.0001)\n",
    "#beta = OLS(X_train, z_train)\n",
    "# Evaluation of model\n",
    "z_mdl = X_test @ beta\n",
    "print(f\"Mean squared error on training data = {mean_squared_error(X_train @ beta, z_train):.4f}\")\n",
    "print(f\"Mean squared error = {mean_squared_error(z_mdl, z_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean squared error on training data = 0.0184\n",
    "Mean squared error = 0.0369\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
