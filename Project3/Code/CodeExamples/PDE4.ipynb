{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the wave equation - the full program using TensorFlow\n",
    "As the program using Autograd is fairly slow, one could hope that using TensorFlow could make a naive implementation faster, and more numerically robust.\n",
    "\n",
    "In addition, having TensorFlow at hand, it could be easier to experiment with different optimization algorithms, and other constructions of the network.\n",
    "\n",
    "The following program solves the given wave equation much faster, \n",
    "\n",
    "https://compphysics.github.io/MachineLearning/doc/pub/odenn/html/._odenn-bs039.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'variable_scope'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0a9b05504111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dnn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mnum_hidden_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_hidden_neurons\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'variable_scope'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "Nx = 10\n",
    "x_np = np.linspace(0,1,Nx)\n",
    "\n",
    "Nt = 10\n",
    "t_np = np.linspace(0,1,Nt)\n",
    "\n",
    "X,T = np.meshgrid(x_np, t_np)\n",
    "\n",
    "x = X.ravel()\n",
    "t = T.ravel()\n",
    "\n",
    "## The construction phase\n",
    "\n",
    "zeros = tf.reshape(tf.convert_to_tensor(np.zeros(x.shape)),shape=(-1,1))\n",
    "x = tf.reshape(tf.convert_to_tensor(x),shape=(-1,1))\n",
    "t = tf.reshape(tf.convert_to_tensor(t),shape=(-1,1))\n",
    "\n",
    "points = tf.concat([x,t],1)\n",
    "\n",
    "num_iter = 100000\n",
    "num_hidden_neurons = [90]\n",
    "\n",
    "X = tf.convert_to_tensor(X)\n",
    "T = tf.convert_to_tensor(T)\n",
    "\n",
    "\n",
    "with tf.variable_scope('dnn'):\n",
    "    num_hidden_layers = np.size(num_hidden_neurons)\n",
    "\n",
    "    previous_layer = points\n",
    "\n",
    "    for l in range(num_hidden_layers):\n",
    "        current_layer = tf.layers.dense(previous_layer, num_hidden_neurons[l],activation=tf.nn.sigmoid)\n",
    "        previous_layer = current_layer\n",
    "\n",
    "    dnn_output = tf.layers.dense(previous_layer, 1)\n",
    "\n",
    "\n",
    "def u(x):\n",
    "    return tf.sin(np.pi*x)\n",
    "\n",
    "def v(x):\n",
    "    return -np.pi*tf.sin(np.pi*x)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    g_trial = (1 - t**2)*u(x) + t*v(x) + x*(1-x)*t**2*dnn_output\n",
    "\n",
    "    g_trial_d2t =  tf.gradients(tf.gradients(g_trial,t),t)\n",
    "    g_trial_d2x = tf.gradients(tf.gradients(g_trial,x),x)\n",
    "\n",
    "    loss = tf.losses.mean_squared_error(zeros, g_trial_d2t[0] - g_trial_d2x[0])\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    traning_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "g_analytic = tf.sin(np.pi*x)*tf.cos(np.pi*t) - tf.sin(np.pi*x)*tf.sin(np.pi*t)\n",
    "g_dnn = None\n",
    "\n",
    "## The execution phase\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(num_iter):\n",
    "        sess.run(traning_op)\n",
    "\n",
    "        # If one desires to see how the cost function behaves during training\n",
    "        #if i % 100 == 0:\n",
    "        #    print(loss.eval())\n",
    "\n",
    "    g_analytic = g_analytic.eval()\n",
    "    g_dnn = g_trial.eval()\n",
    "\n",
    "\n",
    "## Compare with the analutical solution\n",
    "diff = np.abs(g_analytic - g_dnn)\n",
    "print('Max absolute difference between analytical solution and TensorFlow DNN = ',np.max(diff))\n",
    "\n",
    "G_analytic = g_analytic.reshape((Nt,Nx))\n",
    "G_dnn = g_dnn.reshape((Nt,Nx))\n",
    "\n",
    "diff = np.abs(G_analytic - G_dnn)\n",
    "\n",
    "# Plot the results\n",
    "\n",
    "X,T = np.meshgrid(x_np, t_np)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Solution from the deep neural network w/ %d layer'%len(num_hidden_neurons))\n",
    "s = ax.plot_surface(X,T,G_dnn,linewidth=0,antialiased=False,cmap=cm.viridis)\n",
    "ax.set_xlabel('Time $t$')\n",
    "ax.set_ylabel('Position $x$');\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Analytical solution')\n",
    "s = ax.plot_surface(X,T,G_analytic,linewidth=0,antialiased=False,cmap=cm.viridis)\n",
    "ax.set_xlabel('Time $t$')\n",
    "ax.set_ylabel('Position $x$');\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Difference')\n",
    "s = ax.plot_surface(X,T,diff,linewidth=0,antialiased=False,cmap=cm.viridis)\n",
    "ax.set_xlabel('Time $t$')\n",
    "ax.set_ylabel('Position $x$');\n",
    "\n",
    "## Take some 3D slices\n",
    "\n",
    "indx1 = 0\n",
    "indx2 = int(Nt/2)\n",
    "indx3 = Nt-1\n",
    "\n",
    "t1 = t_np[indx1]\n",
    "t2 = t_np[indx2]\n",
    "t3 = t_np[indx3]\n",
    "\n",
    "# Slice the results from the DNN\n",
    "res1 = G_dnn[indx1,:]\n",
    "res2 = G_dnn[indx2,:]\n",
    "res3 = G_dnn[indx3,:]\n",
    "\n",
    "# Slice the analytical results\n",
    "res_analytical1 = G_analytic[indx1,:]\n",
    "res_analytical2 = G_analytic[indx2,:]\n",
    "res_analytical3 = G_analytic[indx3,:]\n",
    "\n",
    "# Plot the slices\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Computed solutions at time = %g\"%t1)\n",
    "plt.plot(x_np, res1)\n",
    "plt.plot(x_np,res_analytical1)\n",
    "plt.legend(['dnn','analytical'])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Computed solutions at time = %g\"%t2)\n",
    "plt.plot(x_np, res2)\n",
    "plt.plot(x_np,res_analytical2)\n",
    "plt.legend(['dnn','analytical'])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Computed solutions at time = %g\"%t3)\n",
    "plt.plot(x_np, res3)\n",
    "plt.plot(x_np,res_analytical3)\n",
    "plt.legend(['dnn','analytical'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
