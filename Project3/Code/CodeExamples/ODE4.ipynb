{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The general program flow in TensorFlow\n",
    "\n",
    "Usually, a program in TensorFlow is divided into two parts; the construction phase and the execution phase. In the construction phase, the computational graph that TensorFlow uses to perform its calculations are set up. In the execution phase, TensorFlow evaluates any procedure that was defined in the construction phase.\n",
    "Program flow in TensorFlow - Construction phase\n",
    "\n",
    "Here, the architecture for the neural network will be set up, along with the cost function and an optimizer class used during training of the network. Note that TensorFlow uses a different convention for the weighting done in each neuron in each layer within the network than in the implementation using Autograd. The matrix-vector multiplication between the input from the previous layer and the weighting at the neuron at current layer in the program using Autograd, is the transpose of the convention used in TensorFlow. But it will not affect that much our construction, as TensorFlow takes care of most of the computations. The only thing we have to be aware of, is how the dimensions are for our inputs.\n",
    "Program flow in TensorFlow - Execution phase\n",
    "\n",
    "The computation graph has been defined, and is ready to be evaluated. In order to get access to the graph, it has to be initialized and be runned within a Session.\n",
    "The full program modeling logistic population growth using TensorFlow \n",
    "\n",
    "https://compphysics.github.io/MachineLearning/doc/pub/odenn/html/._odenn-bs021.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does not work with tensorflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ea98674dc1d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mg_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt_tf\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdnn_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0md_g_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_trial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mg_trial\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mg_trial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         unconnected_gradients)\n\u001b[0m\u001b[0;32m    159\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    489\u001b[0m   \u001b[1;34m\"\"\"Implementation of gradients().\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n\u001b[0m\u001b[0;32m    492\u001b[0m                        \"is enabled. Use tf.GradientTape instead.\")\n\u001b[0;32m    493\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msrc_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Just to reset the graph such that it is possible to rerun this in a\n",
    "# Jupyter cell without resetting the whole kernel.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Set a seed to ensure getting the same results from every run\n",
    "tf.set_random_seed(4155)\n",
    "\n",
    "Nt = 10\n",
    "T = 1\n",
    "t = np.linspace(0,T, Nt)\n",
    "\n",
    "## The construction phase\n",
    "\n",
    "# Convert the values the trial solution is evaluated at to a tensor.\n",
    "t_tf = tf.convert_to_tensor(t.reshape(-1,1),dtype=tf.float64)\n",
    "zeros = tf.reshape(tf.convert_to_tensor(np.zeros(t.shape)),shape=(-1,1))\n",
    "\n",
    "# Define the parameters of the equation\n",
    "alpha = tf.constant(2.,dtype=tf.float64)\n",
    "A = tf.constant(1.,dtype=tf.float64)\n",
    "g0 = tf.constant(1.2,dtype=tf.float64)\n",
    "\n",
    "num_iter = 100000\n",
    "\n",
    "# Define the number of neurons at each hidden layer\n",
    "num_hidden_neurons = [100,50,25]\n",
    "num_hidden_layers = np.size(num_hidden_neurons)\n",
    "\n",
    "# Construct the network.\n",
    "# tf.name_scope is used to group each step in the construction,\n",
    "# just for a more organized visualization in TensorBoard\n",
    "with tf.name_scope('dnn'):\n",
    "\n",
    "    # Input layer\n",
    "    previous_layer = t_tf\n",
    "\n",
    "    # Hidden layers\n",
    "    for l in range(num_hidden_layers):\n",
    "        current_layer = tf.layers.dense(previous_layer, num_hidden_neurons[l], name='hidden%d'%(l+1), activation=tf.nn.sigmoid)\n",
    "        previous_layer = current_layer\n",
    "\n",
    "    # Output layer\n",
    "    dnn_output = tf.layers.dense(previous_layer, 1, name='output')\n",
    "\n",
    "# Define the cost function\n",
    "with tf.name_scope('cost'):\n",
    "    g_trial = g0 + t_tf*dnn_output\n",
    "    d_g_trial = tf.gradients(g_trial,t_tf)\n",
    "\n",
    "    func = alpha*g_trial*(A - g_trial)\n",
    "    cost = tf.losses.mean_squared_error(zeros, d_g_trial[0] - func)\n",
    "\n",
    "\n",
    "# Choose the method to minimize the cost function, along with a learning rate\n",
    "learning_rate = 1e-2\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    traning_op = optimizer.minimize(cost)\n",
    "\n",
    "# Set up a referance to the result from the neural network:\n",
    "g_dnn_tf = None\n",
    "\n",
    "# Define a node that initializes all of the other nodes in the computational graph\n",
    "# used by TensorFlow:\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "## Execution phase\n",
    "\n",
    "# Start a session where the graph defined from the construction phase can be evaluated at:\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the whole graph\n",
    "    init.run()\n",
    "\n",
    "    # Evaluate the initial cost:\n",
    "    print('Initial cost: %g'%cost.eval())\n",
    "\n",
    "    # The training of the network:\n",
    "    for i in range(num_iter):\n",
    "        sess.run(traning_op)\n",
    "\n",
    "        # If one desires to see how the cost function behaves for each iteration:\n",
    "        #if i % 1000 == 0:\n",
    "        #    print(cost.eval())\n",
    "\n",
    "    # Training is done, and we have an approximate solution to the ODE\n",
    "    print('Final cost: %g'%cost.eval())\n",
    "\n",
    "    # Store the result\n",
    "    g_dnn_tf = g_trial.eval()\n",
    "\n",
    "# Compare with analytical solution\n",
    "def get_parameters():\n",
    "    alpha = 2\n",
    "    A = 1\n",
    "    g0 = 1.2\n",
    "    return alpha, A, g0\n",
    "\n",
    "def g_analytic(t):\n",
    "    alpha,A, g0 = get_parameters()\n",
    "    return A*g0/(g0 + (A - g0)*np.exp(-alpha*A*t))\n",
    "\n",
    "g_analytical = g_analytic(t)\n",
    "diff_tf = g_dnn_tf - g_analytical.reshape(-1,1)\n",
    "\n",
    "print('\\nMax absolute difference between the analytical solution and solution from TensorFlow DNN: %g'%np.max(np.abs(diff_tf)))\n",
    "\n",
    "# Plot the result\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.title('Numerical solutions of the ODE')\n",
    "\n",
    "plt.plot(t, g_dnn_tf)\n",
    "plt.plot(t, g_analytical)\n",
    "\n",
    "plt.legend(['dnn, tensorflow', 'exact'])\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('g(t)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
