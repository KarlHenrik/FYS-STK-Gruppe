{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tensorflow.keras.models import Sequential      #This allows appending layers to existing models\n",
    "from tensorflow.keras.layers import Dense           #This allows defining the characteristics of a particular layer\n",
    "from tensorflow.keras import optimizers             #This allows using whichever optimiser we want (sgd,adam,RMSprop)\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "def loss(model, x, t):\n",
    "    with tf.GradientTape() as tape_t:\n",
    "        tape_t.watch([t])\n",
    "\n",
    "        dx_dt = tape_t.gradient(x, t)\n",
    "    #rhs1 = -x + (tf.transpose(x) @ x @ A + (1 - tf.transpose(x) @ A @ x) @ np.eye(6)) @ x\n",
    "    \n",
    "    for i in range(Nt):\n",
    "        x_i = x[6 * i : 6* i + 6]\n",
    "        \n",
    "    rhs1 = -x\n",
    "    rhs2 = x @ tf.transpose(x) @ A\n",
    "    rhs3 = (1 - x @ A @ tf.transpose(x))\n",
    "    rhs = rhs1 + (rhs2 + rhs3) @ x\n",
    "\n",
    "    return tf.losses.mean_squared_error(zeros, dx_dt - rhs)\n",
    "\n",
    "def train(model, optimizer, x, t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss(model, x, t)\n",
    "        \n",
    "    grads = tape.gradient(current_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "# Setting up data\n",
    "Q = np.random.rand(6,6)\n",
    "A = (Q.T + Q)/2\n",
    "\n",
    "Nt = 2\n",
    "x_np = np.random.rand(6,1)\n",
    "t_np = np.linspace(0, 1, Nt)\n",
    "\n",
    "X, T = np.meshgrid(x_np, t_np)\n",
    "\n",
    "x = X.ravel()\n",
    "t = T.ravel()\n",
    "\n",
    "zeros = tf.reshape(tf.convert_to_tensor(np.zeros(x.shape)), shape=(-1,1))\n",
    "x = tf.reshape(tf.convert_to_tensor(x), shape=(-1,1))\n",
    "t = tf.reshape(tf.convert_to_tensor(t), shape=(-1,1))\n",
    "\n",
    "# Setting up model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "model.build(tf.concat([x,t], 1).shape)\n",
    "\n",
    "eta = 0.001\n",
    "sgd = optimizers.SGD(lr=eta)\n",
    "\n",
    "# Training model\n",
    "num_iter = 100000\n",
    "for i in range(num_iter):\n",
    "    train(model, sgd, x, t)\n",
    "    \n",
    "# Output of model\n",
    "g_dnn = (1 - t) * tf.sin(np.pi * x) + x * (1 - x) * t * model(tf.concat([x,t], 1))\n",
    "G_dnn = np.array(g_dnn).reshape((Nt, Nx))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
