{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a8b0da0c1e16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m    315\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[1;32m--> 316\u001b[1;33m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[1;34m(self, loss, var_list, grad_loss)\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m       \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "# Tensorflow/keras\n",
    "from tensorflow.keras.layers import Input, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential      #This allows appending layers to existing models\n",
    "from tensorflow.keras.layers import Dense           #This allows defining the characteristics of a particular layer\n",
    "from tensorflow.keras import optimizers             #This allows using whichever optimiser we want (sgd,adam,RMSprop)\n",
    "from tensorflow.keras import regularizers           #This allows using whichever regularizer we want (l1,l2,l1_l2)\n",
    "from tensorflow.keras.utils import to_categorical   #This allows using categorical cross entropy as the cost function\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "Nx = 10\n",
    "Nt = 10\n",
    "\n",
    "x_np = np.linspace(0, 1, Nx)\n",
    "t_np = np.linspace(0, 1, Nt)\n",
    "\n",
    "X, T = np.meshgrid(x_np, t_np)\n",
    "\n",
    "x = X.ravel()\n",
    "t = T.ravel()\n",
    "\n",
    "## The construction phase\n",
    "\n",
    "zeros = tf.reshape(tf.convert_to_tensor(np.zeros(x.shape)), shape=(-1,1))\n",
    "x = tf.reshape(tf.convert_to_tensor(x), shape=(-1,1))\n",
    "t = tf.reshape(tf.convert_to_tensor(t), shape=(-1,1))\n",
    "\n",
    "points = tf.concat([x,t], 1)\n",
    "\n",
    "num_iter = 100000\n",
    "\n",
    "X = tf.convert_to_tensor(X)\n",
    "T = tf.convert_to_tensor(T)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(90, activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "def u(x):\n",
    "    return tf.sin(np.pi*x)\n",
    "\n",
    "def v(x):\n",
    "    return -np.pi*tf.sin(np.pi*x)\n",
    "\n",
    "def loss(model, x, t):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([x, t])\n",
    "        with tf.GradientTape(persistent=True) as tape_first:\n",
    "            tape_first.watch([x, t])\n",
    "            g_trial = (1 - t**2) * u(x) + t * v(x) + x * (1 - x) * t**2 * model(points)\n",
    "\n",
    "        dg_dx = tape_first.gradient(g_trial, x)\n",
    "        dg_dt = tape_first.gradient(g_trial, t)\n",
    "\n",
    "    dg_d2x = tape.gradient(dg_dx,x)\n",
    "    dg_d2t = tape.gradient(dg_dt,t)\n",
    "\n",
    "    return tf.losses.mean_squared_error(zeros, dg_d2t - dg_d2x)\n",
    "\n",
    "def train(model, x, t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss(model, x, t)\n",
    "    grads = tape.gradient(current_loss, )\n",
    "    optimizer.apply_gradients(zip(grads,[model.W, model.b]))\n",
    "\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "sgd.minimize(loss, x)\n",
    "\n",
    "model.compile(optimizer=sgd)\n",
    "\n",
    "g_analytic = tf.sin(np.pi*x)*tf.cos(np.pi*t) - tf.sin(np.pi*x)*tf.sin(np.pi*t)\n",
    "g_dnn = None\n",
    "\n",
    "## The execution phase\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(num_iter):\n",
    "        sess.run(traning_op)\n",
    "\n",
    "        # If one desires to see how the cost function behaves during training\n",
    "        #if i % 100 == 0:\n",
    "        #    print(loss.eval())\n",
    "\n",
    "    g_analytic = g_analytic.eval()\n",
    "    g_dnn = g_trial.eval()\n",
    "\n",
    "\n",
    "## Compare with the analutical solution\n",
    "diff = np.abs(g_analytic - g_dnn)\n",
    "print('Max absolute difference between analytical solution and TensorFlow DNN = ',np.max(diff))\n",
    "\n",
    "G_analytic = g_analytic.reshape((Nt,Nx))\n",
    "G_dnn = g_dnn.reshape((Nt,Nx))\n",
    "\n",
    "diff = np.abs(G_analytic - G_dnn)\n",
    "\n",
    "# Plot the results\n",
    "\n",
    "X,T = np.meshgrid(x_np, t_np)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Solution from the deep neural network w/ %d layer'%len(num_hidden_neurons))\n",
    "s = ax.plot_surface(X,T,G_dnn,linewidth=0,antialiased=False,cmap=cm.viridis)\n",
    "ax.set_xlabel('Time $t$')\n",
    "ax.set_ylabel('Position $x$');\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Analytical solution')\n",
    "s = ax.plot_surface(X,T,G_analytic,linewidth=0,antialiased=False,cmap=cm.viridis)\n",
    "ax.set_xlabel('Time $t$')\n",
    "ax.set_ylabel('Position $x$');\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Difference')\n",
    "s = ax.plot_surface(X,T,diff,linewidth=0,antialiased=False,cmap=cm.viridis)\n",
    "ax.set_xlabel('Time $t$')\n",
    "ax.set_ylabel('Position $x$');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "x = np.array([\n",
    "    [100, 105.4, 108.3, 111.1, 113, 114.7],\n",
    "    [11, 11.8, 12.3, 12.8, 13.1, 13.6],\n",
    "    [55, 56.3, 57, 58, 59.5, 60.4]\n",
    "])\n",
    "\n",
    "y = np.array([4000, 4200.34, 4700, 5300, 5800, 6400])\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, x, y):\n",
    "        # Initialize variable to (5.0, 0.0)\n",
    "        # In practice, these should be initialized to random values.\n",
    "        self.W = tf.Variable(tf.random.normal((len(x), len(x[0]))))\n",
    "        self.b = tf.Variable(tf.random.normal((len(y),)))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.W * x + self.b\n",
    "\n",
    "\n",
    "def loss(predicted_y, desired_y):\n",
    "    return tf.reduce_sum(tf.square(predicted_y - desired_y))\n",
    "\n",
    "optimizer = tf.optimizers.Adam(0.1)\n",
    "# noinspection PyPep8Naming\n",
    "def train(model, inputs, outputs):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(model(inputs), outputs)\n",
    "    grads = t.gradient(current_loss, [model.W, model.b])\n",
    "    optimizer.apply_gradients(zip(grads,[model.W, model.b]))\n",
    "\n",
    "model = Model(x, y)\n",
    "\n",
    "for i in range(10):\n",
    "    # print(model.b.numpy())\n",
    "    train(model,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
